
## ---------- ETHERERUM

# url = 'https://etherscan.io/token/tokenholderchart/0x72E95b8931767C79bA4EeE721354d6E99a61D004?range=500'

# headers = {
#     'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
#     'accept-language': 'en-GB,en;q=0.9',
#     'cache-control': 'max-age=0',
#     'cookie': 'ASP.NET_SessionId=d212cjpgl5zazkufnnaezfzv; etherscan_offset_datetime=-3; etherscan_switch_token_amount_value=value; etherscan_hidepoortokenalert=True; etherscan_hidezeroalert=True; __stripe_mid=69c4398a-9735-4550-a705-f760fd3a309c7be9ad; __cflb=02DiuFnsSsHWYH8WqVXbZzkeTrZ6gtmGUrqFkD4VCumWt; cf_clearance=mzwrR48P2aRwOw1Dd3g1XdExQajASCtP.60my97Etro-1737263390-1.2.1.1-iWIpA.KnEApkB6388lPQ0W0K.ZW7F4gxUaB7OOTIjzMxxEs6tMF2Gl.q0xatieydswrPJYIFVfDBCWwq_WWeoUrNDzbG2KxX6Mur7Hey73UxYnqcztQu95cVoqiiD_RpR3SJ5WE5MJNZXalch7p.7WAIlZVOf.rf0.GIDUwzFS9tN14j7meq3Kg5SADToc4XvgJVdp0SWi8sCACmz7n0wjmViTbahYTsYJH_f2qpmh4y_QueCJipbONigUCIPNVfuEpbmI_0dWI6bW5JJtPZkhGQfw2XT9ngfSPXu_DR5lidVvpv5pv4jL4ESElGhvs5kWZUBdgCU6KxWFk_uQCzaw',
#     'priority': 'u=0, i',
#     'referer': 'https://etherscan.io/token/tokenholderchart/0x72E95b8931767C79bA4EeE721354d6E99a61D004',
#     'sec-ch-ua': '"Brave";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
#     'sec-ch-ua-arch': '"arm"',
#     'sec-ch-ua-bitness': '"64"',
#     'sec-ch-ua-full-version-list': '"Brave";v="131.0.0.0", "Chromium";v="131.0.0.0", "Not_A Brand";v="24.0.0.0"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-model': '""',
#     'sec-ch-ua-platform': '"macOS"',
#     'sec-ch-ua-platform-version': '"15.2.0"',
#     'sec-fetch-dest': 'document',
#     'sec-fetch-mode': 'navigate',
#     'sec-fetch-site': 'same-origin',
#     'sec-fetch-user': '?1',
#     'sec-gpc': '1',
#     'upgrade-insecure-requests': '1',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
# }

# response = requests.get(url, headers=headers)


## ----------- BASE

# import requests

# url = 'https://basescan.org/token/tokenholderchart/0xD4a0e0b9149BCee3C920d2E00b5dE09138fd8bb7?range=500'
# headers = {
#     'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
#     'accept-language': 'en-GB,en;q=0.5',
#     'cache-control': 'max-age=0',
#     'cookie': 'ASP.NET_SessionId=syjjkhlthnrk2n2vc5nxdhsf; basescan_offset_datetime=-3; basescan_switch_token_amount_value=value; __cflb=02DiuJ1fCRi484mKRwML12UraygpucsySf7Q3gY684tVz; cf_clearance=f2H1ps2b7PSX9SylscTKyFhfrlIue..YFPYvrYUfO5E-1737264686-1.2.1.1-x0TlUDTsM5ixWGLGu8yVkF66xQ8MU.RNShyc_0eE6krZEVkAxksOEsEbh0zhS96VDEdfV1.6zp_o3T2Ft22Wzh14LaKyA9qFkulL51AuE9rHSTnJ9cE0YZufa6WCMFljhSvmgctaSZ31wc8hiqVxNP15c89KNcrERNsRWrvBQ9_mTvmvoMu.IUfnPvORJCPyCjG9zct.a3YxQySVoJpI1Lz7aRAFHjbQFmIJsTvGiMGCs7u_XkMyzFexM1TrBa4ivsGuQcSHQRSgdfdlkBSQrIKnJcVOZBV81_ZWnHhOg.U7HLB02PS1CpMEaleJUJ79OUby0HmskjPKVpmPdU8OxA',
#     'priority': 'u=0, i',
#     'referer': 'https://basescan.org/token/generic-tokenholders2?m=light&a=0xD4a0e0b9149BCee3C920d2E00b5dE09138fd8bb7&s=68578183898687178905981&sid=a647b36e639373d22dd2e64f42ff24ad&p=1',
#     'sec-ch-ua': '"Brave";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
#     'sec-ch-ua-arch': '"arm"',
#     'sec-ch-ua-bitness': '"64"',
#     'sec-ch-ua-full-version-list': '"Brave";v="131.0.0.0", "Chromium";v="131.0.0.0", "Not_A Brand";v="24.0.0.0"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-model': '""',
#     'sec-ch-ua-platform': '"macOS"',
#     'sec-ch-ua-platform-version': '"15.2.0"',
#     'sec-fetch-dest': 'document',
#     'sec-fetch-mode': 'navigate',
#     'sec-fetch-site': 'same-origin',
#     'sec-fetch-user': '?1',
#     'sec-gpc': '1',
#     'upgrade-insecure-requests': '1',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
# }

# response = requests.get(url, headers=headers)


##--------------  Optimism   

# import requests

# url = 'https://optimistic.etherscan.io/token/tokenholderchart/0x77CA01483f379E58174739308945f044e1a764dc?range=500'
# headers = {
#     'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
#     'accept-language': 'en-GB,en;q=0.9',
#     'cache-control': 'max-age=0',
#     'cookie': 'ASP.NET_SessionId=20o34auacn5snbgujxcdrjix; op mainnet etherscan_offset_datetime=-3; op mainnet etherscan_switch_token_amount_value=value; __stripe_mid=69c4398a-9735-4550-a705-f760fd3a309c7be9ad; __cflb=02DiuJ7NLBYKjsZtxjRR4QggQcq1CaL9Q7kUxbnvtjycQ; cf_clearance=bAWAdCKBS9Z1EfvbLTUqyCEbUC0p47GMN34rAJKBAOk-1737264993-1.2.1.1-mOuGv2YgUpgME.LMqhobcNuqrZx7AjDomHvpARWSRBYhcHVL7gtfOR1.vDmhn1d90aPfKhgOlDUGQkxhakaSt.17xRF6RsWSiFh92fI6eaMsj0LvZ76CtRnE1NvWRcEU2OP9k37biPQcEfKRjOAmKRzZ7jATxuqM_nt8oRkkn63tMOHbqVK7uuMGRq6nPlOZ17nryNFNXfH77umUqUdzqUzKM.R3j1VSF9zwMr58oQbeVA25TGMXH1mGuDmw1eGO_1tLiKakOE.06aYzjgVNVv.Ml2CVJ9g7DuAartYNshIRWdbeKO8SLMIeiLZvUc.Co6D6rBegI3k_pcT_jKnCTQ',
#     'priority': 'u=0, i',
#     'referer': 'https://optimistic.etherscan.io/token/tokenholderchart/0x77CA01483f379E58174739308945f044e1a764dc',
#     'sec-ch-ua': '"Brave";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
#     'sec-ch-ua-arch': '"arm"',
#     'sec-ch-ua-bitness': '"64"',
#     'sec-ch-ua-full-version-list': '"Brave";v="131.0.0.0", "Chromium";v="131.0.0.0", "Not_A Brand";v="24.0.0.0"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-model': '""',
#     'sec-ch-ua-platform': '"macOS"',
#     'sec-ch-ua-platform-version': '"15.2.0"',
#     'sec-fetch-dest': 'document',
#     'sec-fetch-mode': 'navigate',
#     'sec-fetch-site': 'same-origin',
#     'sec-fetch-user': '?1',
#     'sec-gpc': '1',
#     'upgrade-insecure-requests': '1',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
# }

# response = requests.get(url, headers=headers)

## ----------------------------------- POLYGON

# import requests

# url = 'https://polygonscan.com/token/tokenholderchart/0x8038857FD47108A07d1f6Bf652ef1cBeC279A2f3?range=500'
# headers = {
#     'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
#     'accept-language': 'en-GB,en;q=0.9',
#     'cache-control': 'max-age=0',
#     'cookie': 'ASP.NET_SessionId=zen30zd42soml15bbdxdv4pt; polygonscan_offset_datetime=-3; polygonscan_switch_token_amount_value=value; __cflb=0H28vYYxgAifymwG4Xt78Bvs2KmKYds75LU77NckDX2',
#     'priority': 'u=0, i',
#     'referer': 'https://polygonscan.com/token/tokenholderchart/0x8038857FD47108A07d1f6Bf652ef1cBeC279A2f3',
#     'sec-ch-ua': '"Brave";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-platform': '"macOS"',
#     'sec-fetch-dest': 'document',
#     'sec-fetch-mode': 'navigate',
#     'sec-fetch-site': 'same-origin',
#     'sec-fetch-user': '?1',
#     'sec-gpc': '1',
#     'upgrade-insecure-requests': '1',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
# }

# response = requests.get(url, headers=headers)


## ------------- ARBITRUM 

# import requests

# url = 'https://arbiscan.io/token/tokenholderchart/0x1aDDD80E6039594eE970E5872D247bf0414C8903?range=500'
# headers = {
#     'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
#     'accept-language': 'en-GB,en;q=0.8',
#     'cache-control': 'max-age=0',
#     'cookie': 'ASP.NET_SessionId=z5tw5gppprxwuogbo0kehcs5; arbitrum one_offset_datetime=-3; arbitrum one_switch_token_amount_value=value; _ga=GA1.1.972759942.1736862088; _ga_KD34MKJY8D=GS1.1.1736862087.1.0.1736862087.0.0.0; cf_clearance=QrPf36WjIxcosACeUCUzJyhN77BS.ebyZGkbX8WxAwM-1737055489-1.2.1.1-kKH2vOYp88TkgUnN20our9UJ1xMm40i3R6vqTPSbahi9rmRddxTIw56SNo0DYgQolcjX3zaKGUnBvLL6XrVAYHXZa5sYENpEZwrD2M9vObvT6jfODu.QXKiV95HgInWfq49NFETccYN0_FtSeCY19NuDTEQKRJFwKWmeUuLhOvAtgBlbhOJgYWn2l3TOk.Id1Axh48W.arGD.G4MMHwbkF3Llvg2br5DHyqHex6YXP5ilp5yjSzVw93BOfs1.EU76P4tPCSgaOci_h7pgZ4tj8H8NAC7TT3.Vy4nKRE2CNUWaL5EJ7dQaD210Sv10cm.xpHO1Iio3rXO6.xpxqJawQ; __cflb=02DiuJLycbJL3fMraug7KdEi8GAxienVNSKUy1xT1iaJY',
#     'priority': 'u=0, i',
#     'referer': 'https://arbiscan.io/token/tokenholderchart/0x1aDDD80E6039594eE970E5872D247bf0414C8903',
#     'sec-ch-ua': '"Brave";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
#     'sec-ch-ua-arch': '"arm"',
#     'sec-ch-ua-bitness': '"64"',
#     'sec-ch-ua-full-version-list': '"Brave";v="131.0.0.0", "Chromium";v="131.0.0.0", "Not_A Brand";v="24.0.0.0"',
#     'sec-ch-ua-mobile': '?0',
#     'sec-ch-ua-model': '""',
#     'sec-ch-ua-platform': '"macOS"',
#     'sec-ch-ua-platform-version': '"15.2.0"',
#     'sec-fetch-dest': 'document',
#     'sec-fetch-mode': 'navigate',
#     'sec-fetch-site': 'same-origin',
#     'sec-fetch-user': '?1',
#     'sec-gpc': '1',
#     'upgrade-insecure-requests': '1',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
# }

# response = requests.get(url, headers=headers)



# # Parse the HTML content
# soup = BeautifulSoup(response.text, 'html.parser')

# # Find the table in the HTML
# table = soup.find('table')

# # Initialize a list to hold the extracted data
# data_list = []

# # Iterate through each row in the table body
# for row in table.tbody.find_all('tr'):
#     # Extract relevant columns (adjust index based on your needs)
#     token_address = row.find_all('td')[2].text.strip()  # Assuming token address is in the third column
#     amount_str = row.find_all('td')[3].text.strip()  # Assuming amount is in the fourth column
    
#     # Handle percentage conversion if applicable
#     if amount_str.endswith('%'):
#         amount = float(amount_str[:-1])  # Remove '%' and convert to float
#     else:
#         amount = float(amount_str.replace(',', ''))  # Convert to float after removing commas

#     data_list.append([token_address, amount])

# # Print the retrieved data
# print(data_list)


# import requests
# from bs4 import BeautifulSoup
# import json
# import re

# def get_token_holder_chart(chain, contract_address, range_value=100, target_date="2023-06-15"):
#     # Define the base URLs for different chains
#     base_urls = {
#         'ethereum': 'https://etherscan.io/token/tokenholderchart/',
#         'optimism': 'https://optimistic.etherscan.io/token/tokenholderchart/',
#         'polygon': 'https://polygonscan.com/token/tokenholderchart/',
#         'arbitrum': 'https://arbiscan.io/token/tokenholderchart/',
#         'base': 'https://basescan.org/token/tokenholderchart/'
#     }

#     # Define the cookies for different chains
#     cookies = {
#         'ethereum': 'ASP.NET_SessionId=d212cjpgl5zazkufnnaezfzv; etherscan_offset_datetime=-780; etherscan_switch_token_amount_value=value; etherscan_hidepoortokenalert=True; etherscan_hidezeroalert=True; __stripe_mid=69c4398a-9735-4550-a705-f760fd3a309c7be9ad; __cflb=02DiuFnsSsHWYH8WqVXbZzkeTrZ6gtmGUrqFkD4VCumWt; cf_clearance=mzwrR48P2aRwOw1Dd3g1XdExQajASCtP.60my97Etro-1737263390-1.2.1.1-iWIpA.KnEApkB6388lPQ0W0K.ZW7F4gxUaB7OOTIjzMxxEs6tMF2Gl.q0xatieydswrPJYIFVfDBCWwq_WWeoUrNDzbG2KxX6Mur7Hey73UxYnqcztQu95cVoqiiD_RpR3SJ5WE5MJNZXalch7p.7WAIlZVOf.rf0.GIDUwzFS9tN14j7meq3Kg5SADToc4XvgJVdp0SWi8sCACmz7n0wjmViTbahYTsYJH_f2qpmh4y_QueCJipbONigUCIPNVfuEpbmI_0dWI6bW5JJtPZkhGQfw2XT9ngfSPXu_DR5lidVvpv5pv4jL4ESElGhvs5kWZUBdgCU6KxWFk_uQCzaw',
#         'optimism': 'ASP.NET_SessionId=20o34auacn5snbgujxcdrjix; op mainnet etherscan_offset_datetime=-3; op mainnet etherscan_switch_token_amount_value=value; __stripe_mid=69c4398a-9735-4550-a705-f760fd3a309c7be9ad; __cflb=02DiuJ7NLBYKjsZtxjRR4QggQcq1CaL9Q7kUxbnvtjycQ; cf_clearance=bAWAdCKBS9Z1EfvbLTUqyCEbUC0p47GMN34rAJKBAOk-1737264993-1.2.1.1-mOuGv2YgUpgME.LMqhobcNuqrZx7AjDomHvpARWSRBYhcHVL7gtfOR1.vDmhn1d90aPfKhgOlDUGQkxhakaSt.17xRF6RsWSiFh92fI6eaMsj0LvZ76CtRnE1NvWRcEU2OP9k37biPQcEfKRjOAmKRzZ7jATxuqM_nt8oRkkn63tMOHbqVK7uuMGRq6nPlOZ17nryNFNXfH77umUqUdzqUzKM.R3j1VSF9zwMr58oQbeVA25TGMXH1mGuDmw1eGO_1tLiKakOE.06aYzjgVNVv.Ml2CVJ9g7DuAartYNshIRWdbeKO8SLMIeiLZvUc.Co6D6rBegI3k_pcT_jKnCTQ',
#         'polygon': 'ASP.NET_SessionId=zen30zd42soml15bbdxdv4pt; polygonscan_offset_datetime=-780; polygonscan_switch_token_amount_value=value; __cflb=0H28vYYxgAifymwG4Xt78Bvs2KmKYds75LU77NckDX2',
#         'arbitrum': 'ASP.NET_SessionId=z5tw5gppprxwuogbo0kehcs5; arbitrum one_offset_datetime=-3; arbitrum one_switch_token_amount_value=value; _ga=GA1.1.972759942.1736862088; _ga_KD34MKJY8D=GS1.1.1736862087.1.0.1736862087.0.0.0; cf_clearance=QrPf36WjIxcosACeUCUzJyhN77BS.ebyZGkbX8WxAwM-1737055489-1.2.1.1-kKH2vOYp88TkgUnN20our9UJ1xMm40i3R6vqTPSbahi9rmRddxTIw56SNo0DYgQolcjX3zaKGUnBvLL6XrVAYHXZa5sYENpEZwrD2M9vObvT6jfODu.QXKiV95HgInWfq49NFETccYN0_FtSeCY19NuDTEQKRJFwKWmeUuLhOvAtgBlbhOJgYWn2l3TOk.Id1Axh48W.arGD.G4MMHwbkF3Llvg2br5DHyqHex6YXP5ilp5yjSzVw93BOfs1.EU76P4tPCSgaOci_h7pgZ4tj8H8NAC7TT3.Vy4nKRE2CNUWaL5EJ7dQaD210Sv10cm.xpHO1Iio3rXO6.xpxqJawQ; __cflb=02DiuJLycbJL3fMraug7KdEi8GAxienVNSKUy1xT1iaJY',
#         'base': 'ASP.NET_SessionId=syjjkhlthnrk2n2vc5nxdhsf; basescan_offset_datetime=-3; basescan_switch_token_amount_value=value; __cflb=02DiuJ1fCRi484mKRwML12UraygpucsySf7Q3gY684tVz; cf_clearance=f2H1ps2b7PSX9SylscTKyFhfrlIue..YFPYvrYUfO5E-1737264686-1.2.1.1-x0TlUDTsM5ixWGLGu8yVkF66xQ8MU.RNShyc_0eE6krZEVkAxksOEsEbh0zhS96VDEdfV1.6zp_o3T2Ft22Wzh14LaKyA9qFkulL51AuE9rHSTnJ9cE0YZufa6WCMFljhSvmgctaSZ31wc8hiqVxNP15c89KNcrERNsRWrvBQ9_mTvmvoMu.IUfnPvORJCPyCjG9zct.a3YxQySVoJpI1Lz7aRAFHjbQFmIJsTvGiMGCs7u_XkMyzFexM1TrBa4ivsGuQcSHQRSgdfdlkBSQrIKnJcVOZBV81_ZWnHhOg.U7HLB02PS1CpMEaleJUJ79OUby0HmskjPKVpmPdU8OxA'
#     }

#     # Validate the chain parameter
#     if chain not in base_urls:
#         raise ValueError(f"Unsupported chain: {chain}")

#     # Construct the URL
#     url = f"{base_urls[chain]}{contract_address}?range={range_value}"

#     # Construct the headers
#     headers = {
#         'date': target_date,
#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
#         'accept-language': 'en-GB,en;q=0.9',
#         'cache-control': 'max-age=0',
#         'cookie': cookies[chain],
#         'priority': 'u=0, i',
#         'referer': url,
#         'sec-ch-ua': '"Brave";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
#         'sec-ch-ua-arch': '"arm"',
#         'sec-ch-ua-bitness': '"64"',
#         'sec-ch-ua-full-version-list': '"Brave";v="131.0.0.0", "Chromium";v="131.0.0.0", "Not_A Brand";v="24.0.0.0"',
#         'sec-ch-ua-mobile': '?0',
#         'sec-ch-ua-model': '""',
#         'sec-ch-ua-platform': '"macOS"',
#         'sec-ch-ua-platform-version': '"15.2.0"',
#         'sec-fetch-dest': 'document',
#         'sec-fetch-mode': 'navigate',
#         'sec-fetch-site': 'same-origin',
#         'sec-fetch-user': '?1',
#         'sec-gpc': '1',
#         'upgrade-insecure-requests': '1',
#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
#     }

#     # Make the request
#     response = requests.get(url, headers=headers)

#     # Parse the HTML content
#     soup = BeautifulSoup(response.text, 'html.parser')

#     # Find the table in the HTML
#     table = soup.find('table')

#     # Initialize a list to hold the extracted data
#     data_list = []

#     # Iterate through each row in the table body
#     for row in table.tbody.find_all('tr'):
#         # Extract relevant columns (adjust index based on your needs)
#         token_address = row.find_all('td')[2].text.strip()  # Assuming token address is in the third column
#         amount_str = row.find_all('td')[3].text.strip()  # Assuming amount is in the fourth column

#         # Handle percentage conversion if applicable
#         if amount_str.endswith('%'):
#             amount = float(amount_str[:-1])  # Remove '%' and convert to float
#         else:
#             amount = float(amount_str.replace(',', ''))  # Convert to float after removing commas

#         data_list.append([token_address, amount])

#     # Print the retrieved data
#     return data_list

# # Example usage
# chain = 'polygon'
# contract_address = '0xc45A479877e1e9Dfe9FcD4056c699575a1045dAA'
# print(get_token_holder_chart(chain, contract_address))


# import streamlit as st
# import plotly.graph_objects as go
# from plotly.subplots import make_subplots
# import pandas as pd
# import json
# from web3 import Web3
# import requests
# from bs4 import BeautifulSoup
# import re


# # Load the JSON data from the GitHub raw URL
# url = 'https://raw.githubusercontent.com/ProtocolCHecker/Liquidity_provider_project/main/dataset_lending.json'

# # Send a GET request to the URL
# response = requests.get(url)
# #url = 'https://raw.githubusercontent.com/ProtocolCHecker/Liquidity_provider_project/refs/heads/main/dataset_lending.json?token=GHSAT0AAAAAAC4XUEG34MDYW6TOR4BWQEEIZ4MUMRQ'
# #response = requests.get(url)

# # Check if the request was successful
# if response.status_code == 200:
#     try:
#         # Try to parse the JSON data
#         assets_data = response.json()
#         print(assets_data)
#         print("JSON data loaded successfully.")
#     except json.JSONDecodeError:
#         # If JSON decoding fails, print the response content
#         print(f"Failed to decode JSON. Response content: {response.content}")
# else:
#     # If the request was not successful, print the status code and response content
#     print(f"Failed to fetch data. Status code: {response.status_code}, Response content: {response.content}")


# import requests
# from bs4 import BeautifulSoup
# import re

# # URL to fetch
# url = 'https://etherscan.io/token/tokenholderchart/0x23878914EFE38d27C4D67Ab83ed1b93A74D4086a'

# # Headers to mimic a browser request
# headers = {
#     'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
#     'accept-language': 'en-GB,en;q=0.8',
#     'cache-control': 'max-age=0',
#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
# }

# # Fetch the webpage
# response = requests.get(url, headers=headers)

# # Check if the request was successful
# if response.status_code == 200:
#     # Parse the HTML content
#     soup = BeautifulSoup(response.content, 'html.parser')

#     # Find all script tags
#     script_tags = soup.find_all('script')

#     js_code = script_tags[-2].text

#     # Regular expression pattern to match Ethereum addresses with optional descriptions
#     pattern = r"\['(0x[a-fA-F0-9]{40}(?: \([^)]+\))?)'"

#     # Find all matches in the JavaScript code
#     matches = re.findall(pattern, js_code)

#     # Print the list of smart contract addresses with details
#     for match in matches:
#         print(match)


# import requests
# import time 

# # List of cryptocurrency symbols and their corresponding CoinGecko IDs
# crypto_symbols = {
#     'Bitcoin': 'bitcoin',
#     'Ethereum': 'ethereum',
#     'Tether': 'tether',
#     'USD Coin': 'usd-coin',
#     'Dai': 'dai',
#     'Optimism': 'optimism',
#     'Aave': 'aave',
#     'Lido Staked Ether': 'lido-staked-ether',
#     'Wrapped stETH': 'wrapped-steth',
#     'WETH': 'weth',
#     'Wrapped Bitcoin': 'wrapped-bitcoin'
# }

# # Fetch and print the price for each cryptocurrency
# for name, symbol in crypto_symbols.items():
#     time.sleep(2)
#     price = requests.get(f'https://api.coingecko.com/api/v3/simple/price?ids={symbol}&vs_currencies=usd').json()[symbol]['usd']
#     print(type(price))
#     print(f"{name}: ${price}")

# import re

# var = "f'https://api.coingecko.com/api/v3/simple/price?ids={symbol}&vs_currencies=usd').json()[symbol]['usd']"
# var = var.strip('"')  # This will remove all double quotes
# print(var)  # Output: fzvr3
